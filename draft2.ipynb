{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ksekinisame apo to baseline\n",
    "meta skeftika na valo ta idia pou eixe gia abstracts kai gia tous authors\n",
    "tora 8a prospa8iso na kano hyperparameter tuning kai na xrisimopoiiso ki alla modela\n",
    "Meta 8a prospa8iso na xrisimopoiiso cosine similrity me tin xrisi to word2vec\n",
    "to cosine similarity den doulevei giati ta vector exoun diaforetika sized kati prepei na allakso\n",
    "\n",
    "den exo kanei kapos to tfidf kai to cosine similarity tou na doulevei den katalavaino giati alla 8a epistrepso sintoma\n",
    "\n",
    "afino stin akri gia ligo ta abstracts kai epikentronomai sto graph -> 8a kano node embedding meta edge embedding\n",
    "telos prepei kapois na xrisimopoiisoume ta features pou dinei to networkx alla einai gtp gt dinei iterators opote prepei na vroume ena scriptaki pou 8a to sozei kapos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import gensim\n",
    "import csv\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Doc2Vec\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499\n",
      "Number of edges: 1091955\n"
     ]
    }
   ],
   "source": [
    "# Create a graph\n",
    "G = nx.read_edgelist('edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "print('Number of nodes:', n)\n",
    "print('Number of edges:', m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read the abstract of each paper\n",
    "abstracts = dict()\n",
    "with open('abstracts.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        node, abstract = line.split('|--|')\n",
    "        abstracts[int(node)] = abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the authors of each paper\n",
    "authors = dict()\n",
    "with open('authors.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        node, author = line.split('|--|')\n",
    "        authors[int(node)] = author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = nx.clustering(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_authors = dict()\n",
    "for node in authors:\n",
    "    for word in word_tokenize(authors[node]):\n",
    "        if word not in stop_words:\n",
    "            if node not in tokenized_authors:\n",
    "                tokenized_authors[node] = [word]\n",
    "            else:\n",
    "                tokenized_authors[node].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_abstracts = dict()\n",
    "for node in abstracts:\n",
    "    tokenized_abstracts[node] = []\n",
    "    for sent in sent_tokenize(abstracts[node]):\n",
    "        for i in word_tokenize(sent):\n",
    "            if i.lower() in stop_words:\n",
    "                continue\n",
    "            else:\n",
    "                tokenized_abstracts[node].append(i.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import countvectorizer and tfidfvectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#author_vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "#author_vectors = author_vectorizer.fit(tokenized_authors.values())\n",
    "sentences = dict()\n",
    "for node in abstracts:\n",
    "    if node not in sentences:\n",
    "        sentences[node] = []\n",
    "    sentences[node].extend(sent_tokenize(abstracts[node]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for node in sentences:\n",
    "    for sent in sentences[node]:\n",
    "        data.append(sent)\n",
    "abstract_vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "abstract_vectors = abstract_vectorizer.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in abstracts:\n",
    "    abstracts[node] = set(abstracts[node].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in authors:\n",
    "    authors[node] = set(authors[node].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_abstract(node):\n",
    "    if sentences[node] == []:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(node):\n",
    "    if has_abstract(node):\n",
    "        return abstract_vectors.transform(sentences[node])\n",
    "    else:\n",
    "        return np.zeros((100, abstract_vectors.vocabulary_.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosim(get_tfidf(136139), get_tfidf(2)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the cosine similarity between the similar words of the abstracts\n",
    "The vectors represent the frequency of the words in each abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "epsilon = 1e-6\n",
    "def counter_cosine_similarity(c1, c2):\n",
    "    terms = set(c1).union(c2)\n",
    "    dotprod = sum(c1.get(k, 0) * c2.get(k, 0) for k in terms)\n",
    "    magA = math.sqrt(sum(c1.get(k, 0)**2 for k in terms))\n",
    "    magB = math.sqrt(sum(c2.get(k, 0)**2 for k in terms))\n",
    "    return dotprod / ((magA * magB) + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = nx.pagerank(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "h,a = nx.hits(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangles = nx.triangles(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_jaccard = list(nx.jaccard_coefficient(G))\n",
    "score, label = zip(*[(s, (u,v) in edge_subset) for (u,v,s) in prediction_jaccard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from node2vec import Node2Vec\n",
    "node2vec = Node2Vec(G, dimensions=100, walk_length=80, num_walks=80, workers=8)\n",
    "n2v_model = node2vec.fit(window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# its class label is 1 if it corresponds to an edge and 0, otherwise.\n",
    "# Use the following 3 features for each pair of nodes:\n",
    "# (1) sum of number of unique terms of the two nodes' abstracts\n",
    "# (2) absolute value of difference of number of unique terms of the two nodes' abstracts\n",
    "# (3) number of common terms between the abstracts of the two nodes\n",
    "# (4) sum of number of unique terms of the two nodes' authors\n",
    "# (5) absolute value of difference of number of unique terms of the two nodes' authors\n",
    "# (6) \n",
    "\n",
    "X_train = np.zeros((2*m, 19))\n",
    "y_train = np.zeros(2*m)\n",
    "n = G.number_of_nodes()\n",
    "for i,edge in enumerate(G.edges()):\n",
    "    # an edge\n",
    "    X_train[i,0] = len(abstracts[edge[0]]) + len(abstracts[edge[1]])\n",
    "    X_train[i,1] = abs(len(abstracts[edge[0]]) - len(abstracts[edge[1]]))\n",
    "    X_train[i,2] = len(abstracts[edge[0]].intersection(abstracts[edge[1]]))\n",
    "    X_train[i,3] = len(authors[edge[0]]) + len(authors[edge[1]])\n",
    "    X_train[i,4] = abs(len(authors[edge[0]]) - len(authors[edge[1]]))\n",
    "    X_train[i,5] = len(authors[edge[0]].intersection(authors[edge[1]]))\n",
    "    X_train[i,6] = counter_cosine_similarity(Counter(tokenized_authors[edge[0]]), Counter(tokenized_authors[edge[1]]))\n",
    "    X_train[i,7] = counter_cosine_similarity(Counter(tokenized_abstracts[edge[0]]), Counter(tokenized_abstracts[edge[1]]))\n",
    "    X_train[i,8] = rank[edge[0]] + rank[edge[1]]\n",
    "    X_train[i,9] = abs(rank[edge[0]] - rank[edge[1]])\n",
    "    X_train[i,10] = cluster[edge[0]] + cluster[edge[1]]\n",
    "    X_train[i,11] = abs(cluster[edge[0]] - cluster[edge[1]])\n",
    "    X_train[i,12] = h[edge[0]] + h[edge[1]]\n",
    "    X_train[i,13] = abs(h[edge[0]] - h[edge[1]])\n",
    "    X_train[i,14] = triangles[edge[0]] + triangles[edge[1]]\n",
    "    X_train[i,15] = abs(triangles[edge[0]] - triangles[edge[1]])\n",
    "    X_train[i,16] = nx.degree(G, edge[0]) + nx.degree(G, edge[1])\n",
    "    X_train[i,17] = abs(nx.degree(G, edge[0]) - nx.degree(G, edge[1]))\n",
    "    X_train[i,18] = nx.clustering(G, edge[0]) + nx.clustering(G, edge[1])\n",
    "    y_train[i] = 1\n",
    "\n",
    "    # a randomly generated pair of nodes\n",
    "    n1 = randint(0, n-1)\n",
    "    n2 = randint(0, n-1)\n",
    "    X_train[m+i,0] = len(abstracts[n1]) + len(abstracts[n2])\n",
    "    X_train[m+i,1] = abs(len(abstracts[n1]) - len(abstracts[n2]))\n",
    "    X_train[m+i,2] = len(abstracts[n1].intersection(abstracts[n2]))\n",
    "    X_train[m+i,3] = len(authors[n1]) + len(authors[n2])\n",
    "    X_train[m+i,4] = abs(len(authors[n1]) - len(authors[n2]))\n",
    "    X_train[m+i,5] = len(authors[n1].intersection(authors[n2]))\n",
    "    X_train[m+i,6] = counter_cosine_similarity(Counter(tokenized_authors[n1]), Counter(tokenized_authors[n2]))\n",
    "    X_train[m+i,7] = counter_cosine_similarity(Counter(tokenized_abstracts[n1]), Counter(tokenized_abstracts[n2]))\n",
    "    X_train[m+i,8] = rank[n1] + rank[n2] # sum of ranks of the two nodes\n",
    "    X_train[m+i,9] = abs(rank[n1] - rank[n2]) # absolute value of difference of ranks of the two nodes\n",
    "    X_train[m+i,10] = cluster[n1] + cluster[n2] # sum of clusters of the two nodes\n",
    "    X_train[m+i,11] = abs(cluster[n1] - cluster[n2]) # absolute value of difference of clusters of the two nodes\n",
    "    X_train[m+i,12] = h[n1] + h[n2] # sum of hubs of the two nodes\n",
    "    X_train[m+i,13] = abs(h[n1] - h[n2]) # absolute value of difference of hubs of the two nodes\n",
    "    X_train[m+i,14] = triangles[n1] + triangles[n2] # sum of triangles of the two nodes\n",
    "    X_train[m+i,15] = abs(triangles[n1] - triangles[n2]) # absolute value of difference of triangles of the two nodes\n",
    "    X_train[m+i,16] = nx.degree(G, n1) + nx.degree(G, n2) # sum of degrees of the two nodes\n",
    "    X_train[m+i,17] = abs(nx.degree(G, n1) - nx.degree(G, n2)) # absolute value of difference of degrees of the two nodes\n",
    "    y_train[m+i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training matrix: (2183910, 16)\n"
     ]
    }
   ],
   "source": [
    "print('Size of training matrix:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((int(t[0]), int(t[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test matrix: (106692, 16)\n"
     ]
    }
   ],
   "source": [
    "# Create the test matrix. Use the same 4 features as above\n",
    "X_test = np.zeros((len(node_pairs), 18))\n",
    "for i,node_pair in enumerate(node_pairs):\n",
    "    X_test[i,0] = len(abstracts[node_pair[0]]) + len(abstracts[node_pair[1]])\n",
    "    X_test[i,1] = abs(len(abstracts[node_pair[0]]) - len(abstracts[node_pair[1]]))\n",
    "    X_test[i,2] = len(abstracts[node_pair[0]].intersection(abstracts[node_pair[1]]))\n",
    "    X_test[i,3] = len(authors[node_pair[0]]) + len(authors[node_pair[1]])\n",
    "    X_test[i,4] = abs(len(authors[node_pair[0]]) - len(authors[node_pair[1]]))\n",
    "    X_test[i,5] = len(authors[node_pair[0]].intersection(authors[node_pair[1]]))\n",
    "    X_test[i,6] = counter_cosine_similarity(Counter(tokenized_authors[node_pair[0]]), Counter(tokenized_authors[node_pair[1]]))\n",
    "    X_test[i,7] = counter_cosine_similarity(Counter(tokenized_abstracts[node_pair[0]]), Counter(tokenized_abstracts[node_pair[1]]))\n",
    "    X_test[i,8] = rank[node_pair[0]] + rank[node_pair[1]]\n",
    "    X_test[i,9] = abs(rank[node_pair[0]] - rank[node_pair[1]])\n",
    "    X_test[i,10] = cluster[node_pair[0]] + cluster[node_pair[1]]\n",
    "    X_test[i,11] = abs(cluster[node_pair[0]] - cluster[node_pair[1]])\n",
    "    X_test[i,12] = h[node_pair[0]] + h[node_pair[1]]\n",
    "    X_test[i,13] = abs(h[node_pair[0]] - h[node_pair[1]])\n",
    "    X_test[i,14] = triangles[node_pair[0]] + triangles[node_pair[1]]\n",
    "    X_test[i,15] = abs(triangles[node_pair[0]] - triangles[node_pair[1]])\n",
    "    X_test[i,16] = nx.degree(G, node_pair[0]) + nx.degree(G, node_pair[1])\n",
    "    X_test[i,17] = abs(nx.degree(G, node_pair[0]) - nx.degree(G, node_pair[1]))\n",
    "print('Size of test matrix:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "logreg = LogisticRegression(C=1e3, solver='liblinear')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = logreg.predict_proba(X_test)\n",
    "y_pred = y_pred[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write predictions to a file\n",
    "predictions = zip(range(len(y_pred)), y_pred)\n",
    "with open(\"submissions.csv\",\"w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
