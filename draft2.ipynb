{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ksekinisame apo to baseline\n",
    "meta skeftika na valo ta idia pou eixe gia abstracts kai gia tous authors\n",
    "tora 8a prospa8iso na kano hyperparameter tuning kai na xrisimopoiiso ki alla modela\n",
    "Meta 8a prospa8iso na xrisimopoiiso cosine similrity me tin xrisi to word2vec\n",
    "to cosine similarity den doulevei giati ta vector exoun diaforetika sized kati prepei na allakso\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import gensim\n",
    "import csv\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Doc2Vec\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499\n",
      "Number of edges: 1091955\n"
     ]
    }
   ],
   "source": [
    "# Create a graph\n",
    "G = nx.read_edgelist('edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "print('Number of nodes:', n)\n",
    "print('Number of edges:', m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read the abstract of each paper\n",
    "abstracts = dict()\n",
    "with open('abstracts.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        node, abstract = line.split('|--|')\n",
    "        abstracts[int(node)] = abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the authors of each paper\n",
    "authors = dict()\n",
    "with open('authors.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        node, author = line.split('|--|')\n",
    "        authors[int(node)] = author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = nx.clustering(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_authors = dict()\n",
    "for node in authors:\n",
    "    for word in authors[node].split(','):\n",
    "        tokenized_authors[node] = [word for word in word_tokenize(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_abstracts = dict()\n",
    "for node in abstracts:\n",
    "    tokenized_abstracts[node] = []\n",
    "    for sent in sent_tokenize(abstracts[node]):\n",
    "        for i in word_tokenize(sent):\n",
    "            if i.lower() in stop_words:\n",
    "                continue\n",
    "            else:\n",
    "                tokenized_abstracts[node].append(i.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Map text to set of terms\n",
    "for node in abstracts:\n",
    "    abstracts[node] = set(abstracts[node].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map authors to set of terms\n",
    "for node in authors:\n",
    "    authors[node] = set(authors[node].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "epsilon = 1e-6\n",
    "def counter_cosine_similarity(c1, c2):\n",
    "    terms = set(c1).union(c2)\n",
    "    dotprod = sum(c1.get(k, 0) * c2.get(k, 0) for k in terms)\n",
    "    magA = math.sqrt(sum(c1.get(k, 0)**2 for k in terms))\n",
    "    magB = math.sqrt(sum(c2.get(k, 0)**2 for k in terms))\n",
    "    return dotprod / ((magA * magB) + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim.models.keyedvectors as word2vec\n",
    "#model = word2vec.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2 = Word2Vec(tokenized_abstracts.values(), window=5, min_count=1, workers=4)\n",
    "from sklearn.cluster import KMeans\n",
    "data = list(tokenized_abstracts.values())\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(data)\n",
    "kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = nx.pagerank(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokenized_abstracts.values()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import countvectorizer and tfidfvectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'development': 1,\n",
       "         'automated': 1,\n",
       "         'system': 3,\n",
       "         'quality': 2,\n",
       "         'assessment': 1,\n",
       "         'aerodrome': 2,\n",
       "         'ground': 1,\n",
       "         'lighting': 1,\n",
       "         'agl': 4,\n",
       "         'accordance': 1,\n",
       "         'associated': 1,\n",
       "         'standards': 2,\n",
       "         'recommendations': 1,\n",
       "         'presented': 2,\n",
       "         'composed': 1,\n",
       "         'image': 4,\n",
       "         'sensor': 1,\n",
       "         'placed': 1,\n",
       "         'inside': 1,\n",
       "         'cockpit': 1,\n",
       "         'aircraft': 1,\n",
       "         'record': 1,\n",
       "         'images': 1,\n",
       "         'normal': 1,\n",
       "         'descent': 1,\n",
       "         'model-based': 1,\n",
       "         'methodology': 1,\n",
       "         'used': 2,\n",
       "         'ascertain': 1,\n",
       "         'optimum': 1,\n",
       "         'match': 1,\n",
       "         'template': 1,\n",
       "         'actual': 1,\n",
       "         'data': 3,\n",
       "         'order': 1,\n",
       "         'calculate': 1,\n",
       "         'position': 2,\n",
       "         'orientation': 2,\n",
       "         'camera': 2,\n",
       "         'instant': 1,\n",
       "         'acquired': 1,\n",
       "         'along': 1,\n",
       "         'pixel': 1,\n",
       "         'grey': 1,\n",
       "         'level': 1,\n",
       "         'imaged': 1,\n",
       "         'luminaire': 3,\n",
       "         'estimate': 1,\n",
       "         'value': 1,\n",
       "         'luminous': 1,\n",
       "         'intensity': 1,\n",
       "         'given': 1,\n",
       "         'compared': 1,\n",
       "         'expected': 1,\n",
       "         'brightness': 1,\n",
       "         'ensure': 1,\n",
       "         'operating': 1,\n",
       "         'required': 1,\n",
       "         'metric': 1,\n",
       "         'pattern': 1,\n",
       "         'determined': 1,\n",
       "         'experiments': 1,\n",
       "         'real': 1,\n",
       "         'demonstrate': 1,\n",
       "         'application': 1,\n",
       "         'effectiveness': 1})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tokenized_abstracts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# its class label is 1 if it corresponds to an edge and 0, otherwise.\n",
    "# Use the following 3 features for each pair of nodes:\n",
    "# (1) sum of number of unique terms of the two nodes' abstracts\n",
    "# (2) absolute value of difference of number of unique terms of the two nodes' abstracts\n",
    "# (3) number of common terms between the abstracts of the two nodes\n",
    "# (4) sum of number of unique terms of the two nodes' authors\n",
    "# (5) absolute value of difference of number of unique terms of the two nodes' authors\n",
    "# (6) \n",
    "\n",
    "X_train = np.zeros((2*m, 12))\n",
    "y_train = np.zeros(2*m)\n",
    "n = G.number_of_nodes()\n",
    "for i,edge in enumerate(G.edges()):\n",
    "    # an edge\n",
    "    X_train[i,0] = len(abstracts[edge[0]]) + len(abstracts[edge[1]])\n",
    "    X_train[i,1] = abs(len(abstracts[edge[0]]) - len(abstracts[edge[1]]))\n",
    "    X_train[i,2] = len(abstracts[edge[0]].intersection(abstracts[edge[1]]))\n",
    "    X_train[i,3] = len(authors[edge[0]]) + len(authors[edge[1]])\n",
    "    X_train[i,4] = abs(len(authors[edge[0]]) - len(authors[edge[1]]))\n",
    "    X_train[i,5] = len(authors[edge[0]].intersection(authors[edge[1]]))\n",
    "    X_train[i,6] = counter_cosine_similarity(Counter(tokenized_authors[edge[0]]), Counter(tokenized_authors[edge[1]]))\n",
    "    X_train[i,7] = counter_cosine_similarity(Counter(tokenized_abstracts[edge[0]]), Counter(tokenized_abstracts[edge[1]]))\n",
    "    X_train[i,8] = rank[edge[0]] + rank[edge[1]]\n",
    "    X_train[i,9] = abs(rank[edge[0]] - rank[edge[1]])\n",
    "    X_train[i,10] = cluster[edge[0]] + cluster[edge[1]]\n",
    "    X_train[i,11] = abs(cluster[edge[0]] - cluster[edge[1]])\n",
    "    y_train[i] = 1\n",
    "\n",
    "    # a randomly generated pair of nodes\n",
    "    n1 = randint(0, n-1)\n",
    "    n2 = randint(0, n-1)\n",
    "    X_train[m+i,0] = len(abstracts[n1]) + len(abstracts[n2])\n",
    "    X_train[m+i,1] = abs(len(abstracts[n1]) - len(abstracts[n2]))\n",
    "    X_train[m+i,2] = len(abstracts[n1].intersection(abstracts[n2]))\n",
    "    X_train[m+i,3] = len(authors[n1]) + len(authors[n2])\n",
    "    X_train[m+i,4] = abs(len(authors[n1]) - len(authors[n2]))\n",
    "    X_train[m+i,5] = len(authors[n1].intersection(authors[n2]))\n",
    "    X_train[m+i,6] = counter_cosine_similarity(Counter(tokenized_authors[n1]), Counter(tokenized_authors[n2]))\n",
    "    X_train[m+i,7] = counter_cosine_similarity(Counter(tokenized_abstracts[n1]), Counter(tokenized_abstracts[n2]))\n",
    "    X_train[m+i,8] = rank[n1] + rank[n2] # sum of ranks of the two nodes\n",
    "    X_train[m+i,9] = abs(rank[n1] - rank[n2]) # absolute value of difference of ranks of the two nodes\n",
    "    X_train[m+i,10] = cluster[n1] + cluster[n2] # sum of clusters of the two nodes\n",
    "    X_train[m+i,11] = abs(cluster[n1] - cluster[n2]) # absolute value of difference of clusters of the two nodes\n",
    "    y_train[m+i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training matrix: (2183910, 12)\n"
     ]
    }
   ],
   "source": [
    "print('Size of training matrix:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((int(t[0]), int(t[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test matrix: (106692, 12)\n"
     ]
    }
   ],
   "source": [
    "# Create the test matrix. Use the same 4 features as above\n",
    "X_test = np.zeros((len(node_pairs), 12))\n",
    "for i,node_pair in enumerate(node_pairs):\n",
    "    X_test[i,0] = len(abstracts[node_pair[0]]) + len(abstracts[node_pair[1]])\n",
    "    X_test[i,1] = abs(len(abstracts[node_pair[0]]) - len(abstracts[node_pair[1]]))\n",
    "    X_test[i,2] = len(abstracts[node_pair[0]].intersection(abstracts[node_pair[1]]))\n",
    "    X_test[i,3] = len(authors[node_pair[0]]) + len(authors[node_pair[1]])\n",
    "    X_test[i,4] = abs(len(authors[node_pair[0]]) - len(authors[node_pair[1]]))\n",
    "    X_test[i,5] = len(authors[node_pair[0]].intersection(authors[node_pair[1]]))\n",
    "    X_test[i,6] = counter_cosine_similarity(Counter(tokenized_authors[node_pair[0]]), Counter(tokenized_authors[node_pair[1]]))\n",
    "    X_test[i,7] = counter_cosine_similarity(Counter(tokenized_abstracts[node_pair[0]]), Counter(tokenized_abstracts[node_pair[1]]))\n",
    "    X_test[i,8] = rank[node_pair[0]] + rank[node_pair[1]]\n",
    "    X_test[i,9] = abs(rank[node_pair[0]] - rank[node_pair[1]])\n",
    "    X_test[i,10] = cluster[node_pair[0]] + cluster[node_pair[1]]\n",
    "    X_test[i,11] = abs(cluster[node_pair[0]] - cluster[node_pair[1]])\n",
    "    \n",
    "print('Size of test matrix:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "logreg = LogisticRegression(C=1e3, solver='liblinear')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = logreg.predict_proba(X_test)\n",
    "y_pred = y_pred[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write predictions to a file\n",
    "predictions = zip(range(len(y_pred)), y_pred)\n",
    "with open(\"submissions2.csv\",\"w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
